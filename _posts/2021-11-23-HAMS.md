---
layout: post
title: "Revamping Storage Class Memory With Hardware Automated Memory-Over-Storage Solution"
author: Jianfeng Wu
tags:
 - ISCA
 - 2021
 - SCM
 - Memory-Over-Storage
---

[论文PDF下载链接](https://arxiv.org/pdf/2106.14241.pdf)

# Introduction

## 持久内存的类型

有三种标准的持久内存类型：NVDIMM-N、NVDIMM-F、NVDIMM-P。

- NVDIMM-N：小型闪存设备和多个带电池的DRAM模块组合在一起，具有类似DRAM的性能，但是容量受限，且Flash部分不可寻址。
- NVDIMM-F：直接将Flash集成到DIMM接口中，提供与存储设备相近的高容量。但是它只暴露了一个块接口，不能替代传统DRAM。
- NVDIMM-P：真正可字节寻址的持久内存（如Optane DC PMM），但性能相比DRAM要差6倍左右。

![](/images/2021-11-23-HAMS/feature_comparison.png)

NVDIMM-N是目前为止唯一具有字节寻址性的DRAM级性能的持久内存，本文中HAMS主要与NVDIMM-N进行对比。

## 持久内存扩展的支持

为了使用SSD扩展NVDIMM，需要软件支持和存储堆栈修改。可以利用mmap来用SSD扩展NVDIMM的持久内存空间。

![](/images/2021-11-23-HAMS/software_support.png)

**mmap流程**
- ① 如果一个进程对SSD调用带有文件描述符(fd)的mmap；
- ② MMF(memory-mapped files)在它的进程地址空间中创建一个新的映射，由一个内存管理结构mm_struct表示（通过分配一个虚拟内存区域VMA给该结构）；
- ③④⑤ 当进程访问VMA指定的内存时，会触发页面错误(如果NVDIMM中没有可用的数据)；
- ⑥ 当发生页面错误时，将调用页面错误处理程序并将一个新页面分配给VMA。由于VMA链接到目标文件，页面错误处理程序检索与fd关联的文件元数据(inode)，为其访问获取一个锁；
- ⑦ MMU与文件系统的错误处理程序交互，从SSD读取页面。文件系统初始化一个块I/O请求结构，称为bio，并将其提交给多队列块I/O队列(blkmq)层，该层通过多个软件队列调度I/O请求；
- ⑧ 队列可以被映射到SSD内一个或多个软件硬件调度队列；
- ⑨ 一旦I/O请求的服务(如bio)完成后,实际的数据加载到一个新的内存分配页面区域,页面错误处理程序创建一个页表条目(PTE),在PTE中记录新的页面地址，然后继续该进程。

上述的扩展持久内存空间方法里，存在页面错误、文件系统、上下文切换及数据复制带来的高开销，可能会抵消超低延迟闪存（ULL-Flash，如Z-NAND）所带来的大部分性能优势。

# Motivation

## ULL-Flash性能表现

要想获得新的研究点，实验测试是必不可少的。作者对Z-SSD原型设备进行了实验评估，分析其性能特点，并与最新的NVMe SSD进行了比较。

![](/images/2021-11-23-HAMS/ULL_vs_NVMe.png)

关键观察：
- ULL-Flash的用户级读/写时延相比DDR4要长330%/79%，相比SSD提升很大，适合用来扩展NVDIMM的持久内存空间。
- IO深度增加时，ULL-Flash保持其延迟特性，而NVMe SSD的延迟会显著增加。
- ULL-Flash读/写带宽相比NVMe SSD提高115%/137%，而且ULL-Flash在IO深度为4时就能达到峰值带宽，NVMe SSD很难达到峰值带宽。
- ULL-Flash仅支持16个未处理的随机读请求，这令NVMe队列管理更加简单。

这些实验除开验证ULL-Flash性能上的优越以外，还证明了使用的NVMe队列可以简化，否则原本完整的NVMe队列所占据的大小是很难做到硬件里去的。（是否说明，随着闪存性能增强，队列深度和数量可以减少，NVMe协议还有优化的空间？）

## 开销分析

作者使用真实设备配置一个基于MMF的主机系统，评估SATA SSD、NVMe SSD和ULL-Flash的性能。结果ULL-Flash显著好于SATA和NVMe（带宽提高399%/118%，延迟降低95%/72%）。

之后又对ULL-Flash的开销进行分析，发现由mmap所引入的系统开销占总执行时间的69%，包括页错误处理、上下文切换、地址转换、边界权限检查等。其中上下文切换是增加延迟的主要原因。

为了消除MMF带来的软件开销，可以绕过存储堆栈，将ULL-Flash模拟为内存模块，直接用ld/st。ULL-Flash相比基于NVDIMM的系统，平均每周期指令数(IPC)降低了98%。原因是负载的数据局部性，很大一部分ld/st指令都受到页面缓存丢失的影响。

# Design

设计目标：从MMF系统中移除mmap和存储堆栈开销；通过在NVDIMM中直接缓存内存引用和自动化ULL-Flash到NVDIMM的映射，来减少stall指令的数量。

核心思想：缓存逻辑的硬件自动化。

![](/images/2021-11-23-HAMS/HAMS_overview.png)

HAMS驻留在内存控制中心MCH中，通过Address manager将ULL-Flash的存储容量暴露给MMU，利用NVDIMM的固定内存空间（避免被无意修改）提供给ULL-Flash的缓存、标签阵列及NVMe协议所需的数据结构（SQ、CQ、BAR。。）等。

# Thought

与FlatFlash的比较：FlatFlash通过SSD内部DRAM作为缓存，直接将ULL-Flash作为字节寻址设备暴露给主机。问题在于设备端DRAM内大部分空间用作地址转换表，剩余空间比主机端DRAM小得多，可能不足以容纳工作集。而且使用MMIO而不是NVMe，不能从SSD内部的并行性中获益。

HAMS发现了MMIO与Flash的特性并不匹配，继续使用NVMe协议作为Flash的访问方式，同时对硬件进行修改使得ULL-Flash也可以对NVDIMM进行内存请求。

比较关键的发现：
- 考虑到闪存内部的并行性，虽然之前很多研究都使用mmap来支持用户空间对闪存的直接访问，但MMIO可能并不是一个很好的访问方式，特别是当访问数据集很大的时候。
- 尽量减少软件堆栈（老生常谈），不使用mmap也有这方面的考虑。
- 可以将协议层做到硬件里？本文实验使用的工具是Gem5，不知道真实硬件做起来会不会存在困难。

[返回博客列表](https://haslab.org/blog/)