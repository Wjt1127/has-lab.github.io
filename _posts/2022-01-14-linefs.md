# [LineFS: Efficient SmartNIC Offload of a Distributed File System with Pipeline Parallelism](https://www.cs.utexas.edu/users/witchel/pubs/kim21sosp-linefs.pdf)

##  问题

 分布式系统的CPU开销一直是影响系统性能的原因，PM（persistent memory）的引入更是加剧这个问题。Assise 和  Orion 都使用了client-local的策略，需要非常多的CPU资源
ss
## Key insight

将存储栈卸载到NIC来降低CPU开销是当下的主流趋势，LineFS将分布式文件系统(dfs)卸载到网卡中，直接将文件系统的相关操作从CPU卸载会降低性能，LineFS为了提高性能做了并行、批处理。异步的优化。

## LineFS的设计
![image](/images/2022-01-14-linefs/Snipaste_2022-01-14_15-16-14.png)


分为装在pm的Host libfs(LineFs client)和在只能网卡的NICFS(lineFS server)，为了降低PM的访问延迟，LineFs采用的是client端本地执行DFS（distributed file system）操作的方式  
LineFS继承了一些Assise的一些设计，包括用户态的PM I/O设计（per-process  LibFSes and update logs），还有引入了zookeeper去管理分布式文件系统的关系

### 提升性能的方法

简单地将文件系统卸载到智能网卡中，由于数据在PCIE间的迁移和并不优秀的智能网卡结构，不会带来多少的提升。
作者提出了persist-and-publish and pipeline parallelism两个方法
**persist-and-publish**  
这里的设计借鉴了strata的设计，persist and publish指的是把数据和元数据的记录放到本地的log中（这里还有点没有想清楚，什么时候进行真正的数据操作，和log的记录顺序），如图一所示， 然后replicate这个log通过智能网卡到远程的PM中。

**Pipeline parallelism**  
pipeline parallelism指的是public和replicate log都利用流水线并行提高效率。  
在节点内部，public过程和replicate过程并发进行，利用不同的流水线，并且保证日志的顺序  
在节点间，不同的lclient节点利用不同pipeline进行并发

### LibFs带来的低延迟PM IO

libFs会结截取posix系统调用，然后更新元数据或者是数据到PM log，因为log的append是顺序的，所以效率会比较高（PM的顺序性能很好）

### DFS pipeline的具体实现

#### publish

**提交日志的原理** 简单的将日志提交给智能网卡存在1）PCIE的高延迟 2）智能网卡性能不足两个挑战  解决方法是persist-and-publish，在应用运行的同时，后台运行publish log到智能网卡，来交叠两个延迟。

**流水线并行**

![image](/images/2022-01-14-linefs/Snipaste_2022-01-14_15-19-43.png)

在publish这个过程可以采用流水线的形式，甚至可以在智能网卡中增加一个级别，进行语义感知压缩的操作

#### replication
replication指的是复制log到远端的PM，利用的是RDMA，fsync()会等到所有的ACK返回后才会结束，来保证一致性。强一致性会导致低性能和很高的CPU占用

**原理**

重新设计一个保证DFS一致性的方法，将轮询卸载到智能网卡的CPU上，保证不影响正在本地运行的应用。

**方法**

RDMA的相关优化

针对低延迟和高带宽的的连接进行不同的处理，还有多路复用RDMA降低QPs（queue pairs）

复制的优化

![image](/images/2022-01-14-linefs/Snipaste_2022-01-14_15-20-36.png)
为了降低延迟，异步进行log entry的复制，和publish一样利用流水线进行加速，前两个阶段和publish是一样的，可以复用

## 测试结果
![image](/images/2022-01-14-linefs/Snipaste_2022-01-14_15-41-30.png)
作者将linefs与Assise进行对比，通过同时运行IO和cpu密集型工作负载，由于资源争用，Assise会使streamcluster在主节点的性能降低72%，在副本中降低66%。DFS客户机运行的主服务器会使用更多的主机CPU和内存资源，导致比副本更严重的减速。Assise- bgrepl提高了Assise的吞吐量18%，限制了与流集群的争用。LineFS显示了最好的吞吐量，比Assise好46%，同时对流集群的影响最小(与单独运行的主节点和副本相比，分别降低了49%和19%)。这一结果证实LineFS的设计在提供良好性能的同时最大限度地减少了主机性能干扰。


## Thought
从内核态文件系统到用户态文件系统  
再从用户态文件系统到智能网卡的lineFs，学界正在重构传统的软件战，降低CPU在其中的占用，这也是由于CPU正愈发成为整体系统瓶颈。之后还有DPU。  
另外一条脉络就是非易性内存的文件系统的发展NOVA strata...之后再整理一下


## 附加
### client-local PM storage 
As PM gains popularity, new DFS designs are appearing that are optimized to leverage PM’s performance. A main design principle of these DFSes is to utilize PM storage that is located on the same machine that executes the applications using it—called client-local PM storage